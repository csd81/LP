

-----

# Chapter 7: MILP Models

In previous chapters, most of the problems we discussed were solvable using **LP (Linear Programming) models**. However, certain complex elements—such as fulfilling an entire order, modeling economies of scale, or including fixed costs—required the use of **integer variables**, resulting in **MILP (Mixed-Integer Linear Programming) models**. That said, the primary problems we looked at (production, diet, and transportation) were fundamentally **continuous** in nature. In those cases, variables could take any value within a range of real numbers, and integer values were only necessary under specific conditions.

In this chapter, we will explore several optimization problems that inherently involve **discrete decisions**. The solution space for these problems isn't a convex, continuous region; instead, it is **finite** (or divided into finite parts). These problems naturally require integer variables—typically **binary variables**—to express discrete choices, which inevitably leads to **MILP models**. Solving these problems is usually significantly more complex than solving problems involving only continuous decisions. Even small instances of MILP problems can be difficult or impossible to solve exhaustively. The exact computational limit depends on the problem itself, the specific MILP model used, the solver software and its configuration, and the hardware running the calculations.

-----

## 7.1 Knapsack Problem

One of the simplest optimization problems that is discrete by nature is the **knapsack problem** [18]. Its definition is as follows:

[Image of knapsack problem illustration]

**Problem 34.**

Given a set of **items**, each having a nonnegative **weight** and a **gain** value, and a known **weight limit**, **select** a subset of the items so that their total weight does not exceed the limit and the total gain is **maximized**.

This is called the knapsack problem because you can visualize it as having a large backpack (knapsack) that you must fill with items. Since the total weight you can carry is limited, you must carefully choose which items to pack to achieve the highest possible value or gain.

As usual, we will describe a concrete knapsack problem instance to illustrate the general problem.

**Problem 35.**

Solve **Problem 34**, the knapsack problem, with a weight **capacity of 60** and the following items:

| **Item** | **A** | **B** | **C** | **D** | **E** | **F** | **G** | **H** | **I** | **J** |
|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| **Weight** | 16.1 | 19.2 | 15.0 | 14.7 | 11.3 | 20.1 | 17.5 | 14.5 | 14.8 | 18.1 |
| **Gain** | 4.4 | 4.9 | 4.3 | 4.0 | 3.7 | 5.1 | 4.6 | 4.2 | 4.3 | 4.8 |

Starting with the GNU MathProg implementation, we first define the **sets** and **parameters** that describe the problem data. The set `Items` denotes the items, and there are two parameters for each item: `Weight` and `Gain`. A single number, `Capacity`, describes the weight limit. We specify all of these as nonnegative to prevent data errors.

```glp
set Items;
param Weight {i in Items}, >=0;
param Gain {i in Items}, >=0;
param Capacity, >=0;
```

Implementing the data section according to Problem 35 using the sets and parameters described above is straightforward.

```glp
set Items := A B C D E F G H I J;
param Weight :=
A 16.1
B 19.2
C 15.0
D 14.7
E 11.3
F 20.1
G 17.5
H 14.5
I 14.8
J 18.1
;
param Gain :=
A 4.4
B 4.9
C 4.3
D 4.0
E 3.7
F 5.1
G 4.6
H 4.2
I 4.3
J 4.8
;
param Capacity := 60;
```

Now, let's look at the flexibility we have in choosing a solution. Any **subset** of the items is a possible solution, including selecting none or all of them. If the number of items is $n$, the total number of different solutions is $2^n$. Note that not all of these subsets are **feasible** due to the weight limit, and we can also eliminate many non-optimal solutions. Nevertheless, the general optimization problem with arbitrary weights and gains is **NP-hard**. Therefore, for a large $n$, finding an exhaustive solution may be practically impossible.

In mathematical programming, we express our decision-making freedom through **decision variables**. We can easily do this using a single **binary variable** for each item. We name it `select`; it equals **1** if the item is chosen for the knapsack and **0** if it is not.

```glp
var select {i in Items}, binary;
```

There is only one restriction that can prevent a solution from being feasible: the **total weight** of the items. We express this in a single constraint stating that the total weight of the selected items is at most the given limit. The total weight in the knapsack is calculated by summing each `select` variable multiplied by the item's `Weight`. This works because if an item is selected, its weight is added; if it's not selected, it contributes nothing to the total.

```glp
s.t. Total_Weight:
sum {i in Items} select[i] * Weight[i] <= Capacity;
```

The objective is to **maximize** the **total gain**. We sum each `select` variable multiplied by the item's `Gain` value.

```glp
maximize Total_Gain:
sum {i in Items} select[i] * Gain[i];
```

After the `solve` statement, we add some print commands to display the solution. Our complete model section is now ready:

```glp
set Items;
param Weight {i in Items}, >=0;
param Gain {i in Items}, >=0;
param Capacity, >=0;
var select {i in Items}, binary;
# var select {i in Items}, >=0, <=1;
s.t. Total_Weight:
sum {i in Items} select[i] * Weight[i] <= Capacity;
maximize Total_Gain:
sum {i in Items} select[i] * Gain[i];
solve;
printf "Optimal Gain: %g\n", Total_Gain;
printf "Total Weight: %g\n",
sum {i in Items} select[i] * Weight[i];
for {i in Items}
{
printf "%s:%s\n", i, if (select[i]) then " SELECTED" else "";
}
end;
```

Solving Problem 35 reports an **optimal gain of 17.1**, achieved by selecting items **C, E, I, and J**. The total weight of these items is **59.2**, which fits within the limit of 60, though it doesn't utilize the capacity perfectly.

Note that we used a **conditional expression** to print the word `SELECTED` after each item $i$ where `select[i]` is 1, and an empty string where it is 0. The condition for such an expression must be a constant expression that can be interpreted as a logical value. Remember that variables only behave as constants *after* the `solve` statement in the model section. The output produced is as follows:

```
Optimal Gain: 17.1
Total Weight: 59.2
A:
B:
C: SELECTED
D:
E: SELECTED
F:
G:
H:
I: SELECTED
J: SELECTED
```

You might think of an easy, direct **heuristic** for the knapsack problem: arrange the items in descending order of their **gain-to-weight ratio** and select items in this order until you hit the weight limit. This **greedy strategy** seems promising because the weight limit acts as a fixed resource, and we want to maximize the gain per unit of weight. "Greedy" means that at each step, we make the most immediately beneficial choice based on a heuristic. Items with a higher gain/weight ratio represent a more efficient use of the capacity.

The only issue with this strategy is the **slack weight**. We may (and usually will) have some unused weight under the limit. We might actually achieve a better overall solution by sacrificing an item with a high ratio to better fill the remaining space with other items. This small difference prevents the straightforward heuristic from being optimal and makes the knapsack problem computationally difficult.

Let's consider a **"relaxed" version** of the knapsack problem where the items are fluids. This means we are allowed to select a **fraction** of an item, receiving the same fraction of its weight and gain. For this relaxation, the binary variable can be replaced by a continuous one, as follows:

```glp
var select {i in Items}, >=0, <=1;
```

Note that you can achieve this effect by running the original model with the `glpsol` solver and adding the `--nomip` option:

```bash
glpsol -m knapsack.mod -d example.dat --nomip
```

Furthermore, we'll change the output method slightly. Instead of printing the word `SELECTED`, we'll print the actual value of the now continuous `select` variable:

```glp
for {i in Items}
{
printf "%s: %g\n", i, select[i];
}
```

Solving the relaxed model with the exact same data gives the following results:

```
Optimal Gain: 17.7025
Total Weight: 60
A: 0.273292
B: 0
C: 1
D: 0
E: 1
F: 0
G: 0
H: 1
I: 1
J: 0
```

Notice that items C, E, and I are selected entirely. However, instead of choosing J as the fourth item (as in the integer solution), the relaxed model chooses H completely and then item A in a **fractional ratio** (approximately $27.3\%$). This results in perfect utilization of the weight limit (60) and a slightly better objective value of **17.7025** compared to 17.1. If we analyze the data, the items' decreasing order of **gain/weight ratio** is E, I, H, C, A, D, J, G, B, F. Therefore, the relaxed model performs exactly as the greedy heuristic dictates: it selects the first four items entirely and then the necessary fraction of the fifth (A) to fill the limit.

In contrast, the optimal solution to the actual **integer programming problem** does not choose H. Instead, it chooses J, which is much lower in the ratio order, because J fills the remaining weight limit better due to its larger overall weight and gain.

Now that we have seen the complications that arise with integer problems, let's consider a similar problem: the **multi-way number partitioning problem** [19].

**Problem 36.**

Given a set of $N \ge 1$ **real numbers**, divide them **exhaustively** into exactly $K \ge 1$ **subsets** so that the difference between the **smallest** and **largest** sum of numbers in a subset is **minimal**.

The **multi-way number partitioning problem** can be interpreted as a modified version of the **knapsack problem** where all $N$ **items** must be packed into any one of $K$ given **knapsacks**, and this distribution must be as **balanced** as possible. There is no **gain value** here (or rather, the gain can be interpreted as being equal to the **weight**).

We will solve one example problem.

-----

**Problem 37.**

**Distribute** the $N=10$ **items** described in Problem 35 (the **knapsack example problem**) into $K=3$ **knapsacks** so that the difference between the **lightest** and the **heaviest** knapsack is **minimal**.

Let's see how to **implement** this in **GNU MathProg**. First, there is an `Items` set and a `Weight` parameter, just as before, but no `Gain` parameter. Instead, we **define** a single integer parameter `Knapsack_Count`, which refers to the **positive integer** $K$ from the problem description.

```
set Items;
param Weight {i in Items}, >=0;
param Knapsack_Count, integer;
set Knapsacks := 1 .. Knapsack_Count;
```

We also introduced the set `Knapsacks`. However, instead of reading this set from the data section, we **denote** each knapsack by numbers from **1 to K**. The operator `..` defines a set by listing all integers between the **smallest** and **largest** integer element.

Note that we require `Weight` to be **non-negative**; however, these parameters can be restricted to **integers** or relaxed to take **real values**. The **model** remains exactly the same in all cases.

There are **more decisions** to be made here than in the original knapsack problem. For each item, we don't just decide *whether* it goes into a knapsack; we decide **which knapsack it goes into**. We can do this by defining a **binary decision variable** `select` for each pair of an **item** and a **knapsack**, denoting whether that item goes into that specific knapsack. While these decisions cover the situation, we need **auxiliary variables** to express the **objective function concisely**. Therefore, we introduce a variable `weight` for each knapsack (denoting its total weight), along with `min_weight` and `max_weight` for the overall minimal and maximal knapsack weights.

```
var select {i in Items, k in Knapsacks}, binary;
var weight {k in Knapsacks};
var min_weight;
var max_weight;
```

There is only **one constraint** that determines if a decision regarding knapsacks is **feasible**: **each item must go exactly into one knapsack**. If we sum the binary variables for a specific item across all knapsacks and set that sum to one, the only feasible solution is for **one binary variable to be 1 and all others to be 0**. Therefore, the constraint is:

```
s.t. Partitioning {i in Items}:
sum {k in Knapsacks} select[i,k] = 1;
```

We provide **three additional constraint statements** to express the calculation of each knapsack's weight, a **lower limit** on all knapsack weights (`min_weight`), and an **upper limit** (`max_weight`).

```
s.t. Total_Weights {k in Knapsacks}:
weight[k] = sum {i in Items} select[i,k] * Weight[i];
s.t. Total_Weight_from_Below {k in Knapsacks}:
min_weight <= weight[k];
s.t. Total_Weight_from_Above {k in Knapsacks}:
max_weight >= weight[k];
```

The **objective** is the **difference** between the upper and lower limits.

```
minimize Difference: max_weight - min_weight;
```

This design has been used before for **minimizing errors in equations** (Section 4.8), **maximizing minimum production volumes** (Section 5.3), and various **cost functions** (Sections 6.3 or 6.6). The key is that the solver is allowed **not** to assign the actual **minimum** and **maximum** weights to the variables `min_weight` and `max_weight` while still finding **feasible solutions**. However, it is **not beneficial** to do so. Therefore, solutions where these two bounds are not **strict** are **automatically ruled out** during minimization.

Finally, we **print the contents of each knapsack** after the `solve` statement, and our model section is **ready**.

```
set Items;
param Weight {i in Items}, >=0;
param Knapsack_Count, integer;
set Knapsacks := 1 .. Knapsack_Count;
var select {i in Items, k in Knapsacks}, binary;
var weight {k in Knapsacks};
var min_weight;
var max_weight;
s.t. Partitioning {i in Items}:
sum {k in Knapsacks} select[i,k] = 1;
s.t. Total_Weights {k in Knapsacks}:
weight[k] = sum {i in Items} select[i,k] * Weight[i];
s.t. Total_Weight_from_Below {k in Knapsacks}:
min_weight <= weight[k];
s.t. Total_Weight_from_Above {k in Knapsacks}:
max_weight >= weight[k];
minimize Difference: max_weight - min_weight;
solve;
printf "Smallest difference: %g (%g - %g)\n",
Difference, max_weight, min_weight;
for {k in Knapsacks}
{
printf "%d:", k;
for {i in Items: select[i,k]}
{
printf " %s", i;
}
printf " (%g)\n", weight[k];
}
end;
```

Solving example **Problem 37** gives the following result:

```
Smallest difference: 2.5 (55.3 - 52.8)
1: C F J (53.2)
2: D E H I (55.3)
3: A B G (52.8)
```

The **ten items** could be divided into **three subsets** of roughly **equal size**. The **largest knapsack** is the second (weight **55.3**), and the **smallest** is the third (weight **52.8**). Note that the **order of knapsacks** is **not important**. Because **all ten items are distributed**, it is **guaranteed** that the sum of the three knapsacks is a **constant**; therefore, their **average** is also constant and must lie between the two limits.

Another similar and **better-known problem** is the **bin packing problem**, where all items are known, but the **knapsack sizes are fixed**. In that case, the goal is to **minimize** the **number of knapsacks** (called **bins**) [20]. This could also be solved in **GNU MathProg**, but we won't detail it here.

-----

## 7.2 Tiling the Grid

The **knapsack** and **similar problems** require items to "fit" somewhere based on weight constraints. But what happens when **fitting is more complex**? For instance, considering the **size** or **shape** of items and their **container** is a **common real-world question** that leads to much more **difficult optimization problems**.

A **two-dimensional example** is the class of **tiling problems**, where copies of the **same shape** (called **tiles**) are used to **cover a region** on a plane. Tiles are usually **forbidden to overlap**, and the **cover** is often required to be **perfect**; that is, **all of the designated region must be covered**. If we don't require **perfect covering**, we might ask: what is the **maximum area we can cover**? This becomes an **optimization problem**. Provided there is a **single tile**, the copies of which are used, the problem simplifies to **maximizing the number of non-overlapping tiles** that can be put into the region.

In this section, we **restrict tiling** to a **rectangular grid** of **unit squares**. The **tile**, **all its possible positions**, and the **region to be covered** all fit onto **unit squares** of the same grid. The tile will be the **simplest cross**, consisting of **five squares**. For simplicity, the **region to be covered** is a **rectangular area** (see Figure 8). Tiles cannot cover any area outside this region.

**Problem 38.**

Determine the **maximum number of non-overlapping crosses** that can be placed in an $N \times M$ grid.

You might argue that this is a very specialized problem within the class of **tiling problems**. Indeed, many tiling problems cannot be effectively solved using mathematical programming tools. Here, the "general problem" simply refers to creating a general model section that can be applied to any instance of this grid problem.

The data required for this problem is minimal: only the dimensions of the rectangular area need to be specified. Let's look at some examples.

**Problem 39.**

Solve **Problem 38**, the rectangle tiling problem with crosses, for the following rectangle sizes:

  * $6 \times 4$
  * $10 \times 10$
  * $30 \times 30$

This problem requires perhaps the shortest data sections we've seen so far. The following is for the smallest instance:

```glp
param Width := 6;
param Height := 4;
```

We use the parameter names **`Width`** and **`Height`** for the dimensions of the rectangular region. These must be positive integers. **`Height`** represents $N$ (the number of rows), and **`Width`** represents $M$ (the number of columns). Rows are numbered from 1 to $N$, and columns are numbered from 1 to $M$. The set of all squares, or **cells**, in the rectangular grid is the Cartesian product of the row and column sets, resulting in a size of $N \cdot M$. These are defined as shown below.

```glp
param Height, integer, >=1;
param Width, integer, >=1;
set Rows := 1 .. Height;
set Cols := 1 .. Width;
set Cells := Rows cross Cols;
```

At this point, the minimum data requirements are defined in the model, but a concise implementation will require a few more definitions. Let's first determine the decisions needed to describe a tiling.

First, we need to decide, for every possible position of a cross tile, whether to put a tile there or not. This can be done using a **binary variable** introduced for each possible position:

```glp
var place {(r,c) in Tiles}, binary;
```

Here, the set **`Tiles`** refers to a set that hasn't been defined yet, but it will list all possible placements of cross tiles. Decisions for this `place` variable will fully determine the tiling, but an **auxiliary variable** will also be useful.

```glp
var covered {(r,c) in Cells}, binary;
```

Here, `covered` is defined for each cell and determines whether that cell is occupied by a tile. This helps us formulate constraints ensuring that each cell is covered **at most once** to prevent overlaps.

As is customary, a value of **1 means yes** and **0 means no** for these binary variables. For instance, `place` is 1 if a tile is placed at that specific position, and `covered` is 1 if that specific cell is occupied.

Before moving on, we must characterize the set **`Tiles`** that describes all possible positions. Fortunately, the five-cell cross is **symmetric**: rotating or reflecting it does not change its orientation. This means the only difference between tile positions is their location (shifting). Therefore, we define a binary variable for each possible position, and the set `Tiles` simply lists these positions.

If we wanted to tile with **asymmetric shapes** where multiple orientations are possible, we would need to define different binary variables for each unique orientation and position. In that scenario, the `Tiles` set would require a third index dimension to denote orientation. But since the cross only has a single orientation, that dimension is omitted, and `Tiles` is just a two-dimensional set.

Finally, let's decide how coordinates will define the positioning. We define the **anchor point** of the cross tile as the **central cell** of the tile. The set `Tiles` will list all possible anchor points of correctly positioned tiles. This works because different positions have different anchor points.

The selection of the anchor point relative to the tile can be arbitrary, as long as it is consistent. It could even be a cell outside the tile—for example, the corner cell of the $3 \times 3$ square containing the cross.

The last task is to determine the valid anchor points. We must consider all correctly positioned cross tiles in the grid. Since the anchor point is inside the tile, the anchor of a valid tile must be within the grid. However, **not all cells** in the grid are valid anchors. For example, no cells along the edge of the rectangle are valid, as a cross centered there would extend outside the region. Furthermore, corner cells cannot be covered by *any* correctly positioned cross tiles centered elsewhere.

Based on an anchor $(r,c)$, we can define all the cells of the cross tile if placed at that anchor. This is done by the **`CellsOf`** set. Note that this set statement is itself indexed over all cells (i.e., all possible anchors), meaning that $N \times M$ different sets of 5 cells each are defined.

```glp
set CellsOf {(r,c) in Cells} :=
{(r,c),(r+1,c),(r-1,c),(r,c+1),(r,c-1)};
```

The set **`Tiles`** of correct tile positions (anchor points) consists of those anchors for which the 5 cells defined in `CellsOf` are all within the rectangular area.

```glp
set Tiles, within Cells :=
setof {(r,c) in Cells: CellsOf[r,c] within Cells} (r,c);
```

Of course, we could have simply said correct anchor points are those **not on the edges**. However, we chose this approach for two reasons:

  * Defining all cells of the placed tile and keeping only positions entirely within the region is a very **general approach**. It works for arbitrary tiles and arbitrary regions on a finite grid.
  * The `CellsOf` sets provide a **shorter model formulation**, as we will see.

The definitions for `CellsOf` and `Tiles` must be placed before the definition of the variable `place`.

Constraints must establish two things: calculation of the auxiliary variable `covered` for each cell, and ensuring each cell is covered at most once. Surprisingly, both can be done with a single constraint statement:

```glp
s.t. No_Overlap {(r,c) in Cells}: covered[r,c] =
sum {(x,y) in Tiles: (r,c) in CellsOf[x,y]} place[x,y];
```

In plain terms, for each cell, we add the `place` variables of all tiles that cover that cell. Since `place` is an integer variable, the sum exactly equals the number of tiles covering that specific cell. This number can only be zero or one; two or more is forbidden. This restriction is implicitly ensured because the sum equals the single binary variable `covered[r,c]`, and since it is binary, its value cannot exceed one.

Note that `covered` doesn't strictly need to be formulated as **binary**. The constraint ensures its value is an integer because it's a sum of integer variables. The only required property is its **upper bound of 1**.

This insight is useful for analyzing complexity. Generally, more binary variables make a model harder to solve. However, since `covered` doesn't strictly have to be binary, it isn't expected to significantly increase complexity. The true difficulty lies with the `place` variable. Nevertheless, marking `covered` as binary might alter the solution algorithm's process.

The objective is the total number of tiles placed.

```glp
maximize Number_of_Crosses:
sum {(r,c) in Tiles} place[r,c];
```

After the `solve` statement, a useful way to print the solution is to visualize the tiling using character graphics.

```glp
for {r in Rows}
{
for {c in Cols}
{
printf "%s",
if (!covered[r,c]) then "."
else if ((r,c) in Tiles) then (if (place[r,c]) then "#" else "+")
else "+";
}
printf "\n";
}
```

Since the output is textual, we print each row on a single line. Within a row, we print a single character for each cell so the area aligns correctly in a fixed-width font:

  * If a cell is **not covered**, a **dot (`.`)** is placed there.
  * If a cell is the anchor point of a **placed tile**, it is denoted by a **hash mark (`#`)**.
  * Otherwise, if the cell is covered by a cross but is **not its center**, it is denoted by a **plus sign (`+`)**.

Note the use of nested `if` operators. The key point is that we first check whether $(r,c)$ is a proper anchor point. Only if it is an anchor point do we check `place[r,c]`. We do this because referring to `place[r,c]` for a non-anchor point would cause an **"out of domain" error**, as the `place` variable is only defined for members of the set `Tiles`. Note that if an `else` value is omitted in MathProg, it is assumed to be zero.

The model section for **Problem 38** is complete.

```glp
param Height, integer, >=1;
param Width, integer, >=1;
set Rows := 1 .. Height;
set Cols := 1 .. Width;
set Cells := Rows cross Cols;
set CellsOf {(r,c) in Cells} :=
{(r,c),(r+1,c),(r-1,c),(r,c+1),(r,c-1)};
set Tiles, within Cells :=
setof {(r,c) in Cells: CellsOf[r,c] within Cells} (r,c);
var covered {(r,c) in Cells}, binary;
var place {(r,c) in Tiles}, binary;
s.t. No_Overlap {(r,c) in Cells}: covered[r,c] =
sum {(x,y) in Tiles: (r,c) in CellsOf[x,y]} place[x,y];
maximize Number_of_Crosses:
sum {(r,c) in Tiles} place[r,c];
solve;
printf "Max. Cross Tiles (%dx%d): %g\n",
Height, Width, Number_of_Crosses;
for {r in Rows}
{
for {c in Cols}
{
printf "%s",
if (!covered[r,c]) then "."
else if ((r,c) in Tiles) then (if (place[r,c]) then "#" else "+")
else "+";
}
printf "\n";
}
end;
```

Solving the model for the smallest instance ($4 \times 6$) shows that no more than **two** cross tiles can fit. One possible construction is shown below:

```
Max. Cross Tiles (4x6): 2
....+.
.+.+#+
+#+.+.
.+....
```

The medium instance ($10 \times 10$) is still solved very quickly. A maximum of **13** cross tiles fit, and the solution looks like this:

```
Max. Cross Tiles (10x10): 13
.+...+..+.
+#+.+#++#+
.++..+.++.
.+#+.++#+.
..+++#+++.
.++#++++#+
+#++.+#++.
.++..++.+.
.+#++#++#+
..+..+..+.
```

The large instance ($30 \times 30$) was included to demonstrate what happens when the model becomes genuinely large and difficult to solve. In such cases, the solver could take an unacceptably long time. Therefore, we provide a **time limit of 60 seconds**. In the command line, add the `--tmlim 60` argument:

```bash
glpsol -m tiling.mod -d example.dat --tmlim 60
```

If the time limit option is omitted, the solver runs indefinitely. Note that the limit set this way is not strict; `glpsol` tends to slightly exceed it, especially if preparatory steps are lengthy.

Running `glpsol` with a one-minute time limit produced the following output:

```
GLPK Integer Optimizer, v4.65
901 rows, 1684 columns, 5604 non-zeros
1684 integer variables, all of which are binary
Preprocessing...
896 rows, 1680 columns, 4816 non-zeros
1680 integer variables, all of which are binary
Scaling...
A: min|aij| = 1.000e+00 max|aij| = 1.000e+00 ratio = 1.000e+00
Problem data seem to be well scaled
Constructing initial basis...
Size of triangular part is 896
Solving LP relaxation...
GLPK Simplex Optimizer, v4.65
896 rows, 1680 columns, 4816 non-zeros
*
0: obj = -0.000000000e+00 inf = 0.000e+00 (784)
Perturbing LP to avoid stalling [253]...
Removing LP perturbation [1896]...
* 1896: obj = 1.631480829e+02 inf = 0.000e+00 (0) 12
OPTIMAL LP SOLUTION FOUND
```

Note that the original model contained a massive **1684 binary variables**. Preprocessing only removed four of them (likely the corner cells).

After that, the **LP relaxation** of the MILP model is solved. The rows concerning "perturbation" indicate that the solver took measures to avoid **stalling** (an infinite loop in the Simplex algorithm). This message suggests that the solved LP is difficult, very large, or has unusual properties. The last line shows that **163.15** was the optimal solution of the LP relaxation. This tells us that the optimal solution of the actual MILP model **cannot be greater than 163**. The algorithm uses this result to set an initial bound.

```
Integer optimization begins...
Long-step dual simplex will be used
+ 1896: mip = not found yet <= +inf (1; 0)
+ 4865: mip = not found yet <= 1.630000000e+02 (44; 0)
+ 9025: mip = not found yet <= 1.630000000e+02 (102; 0)
+ 11252: >>>>> 1.380000000e+02 <= 1.630000000e+02 18.1% (176; 0)
+ 14948: mip = 1.380000000e+02 <= 1.620000000e+02 17.4% (200; 35)
+ 18811: mip = 1.380000000e+02 <= 1.620000000e+02 17.4% (266; 35)
+ 22402: mip = 1.380000000e+02 <= 1.620000000e+02 17.4% (330; 36)
+ 25362: mip = 1.380000000e+02 <= 1.620000000e+02 17.4% (384; 36)
+ 28609: mip = 1.380000000e+02 <= 1.620000000e+02 17.4% (457; 36)
+ 29844: >>>>> 1.400000000e+02 <= 1.620000000e+02 15.7% (484; 36)
+ 33482: mip = 1.400000000e+02 <= 1.620000000e+02 15.7% (492; 108)
+ 36688: mip = 1.400000000e+02 <= 1.620000000e+02 15.7% (557; 108)
+ 40050: mip = 1.400000000e+02 <= 1.620000000e+02 15.7% (639; 109)
+ 43326: mip = 1.400000000e+02 <= 1.620000000e+02 15.7% (687; 109)
Time used: 60.0 secs. Memory used: 6.4 Mb.
+ 43866: mip = 1.400000000e+02 <= 1.620000000e+02 15.7% (698; 109)
TIME LIMIT EXCEEDED; SEARCH TERMINATED
Time used: 60.3 secs
Memory used: 6.7 Mb (7071653 bytes)
```

The integer optimization follows, generally executed using a **Branch and Bound** procedure. Branching checks different integer values, and bounding eliminates branches early.

Output rows are printed regularly:

  * The numbers on the left (ending at 43866) denote the "steps" taken.
  * The middle column is the currently known **best integer solution** (or "MIP," Mixed-Integer Program). This value can only improve (increase) during the search.
      * Initially, no solution was found.
      * The first solution found involved **138 tiles**.
      * In 60 seconds, this improved to **140 tiles**, at which point the time limit stopped the process.
  * The rightmost column denotes the current **best bound**. This is an upper limit on the objective. It can only decrease. It started at 163 and improved slightly to 162.

This means that even though the procedure didn't finish, there definitely cannot be more than **162 tiles**.

The optimal solution lies somewhere between the best solution and the best bound. If they become equal, the optimal solution is found. The relative difference between them is the **integrality gap** (here, $15.7\%$).

Concluding the large case: we have a feasible solution of **140 tiles**, but the true optimal solution could be as high as **162**. It is possible that 140 is the optimal solution, but the solver hasn't proven it.

What are our options for a large model that `glpsol` fails to solve optimally?

  * **Spend More Time:** Improvement usually slows down, so this might take too long.
  * **Formulate a Better Model:** We can try different decision variables or **tightening constraints** (like cutting planes) to help the solver. A better formulation can drastically improve efficiency.
  * **Choose a Different Solver or Machine:** There are more powerful solvers than `glpsol` (e.g., CBC, lpsolve, or commercial solvers). `glpsol` can export models to CPLEX-LP format for use with these tools.
  * **Adjust Solver Options:** `glpsol` has options that alter the algorithms, such as enabling heuristics.

We will demonstrate the last option: running `glpsol` again with two heuristic options enabled:

  * **Feasibility Pumping** (`--fpump`): A heuristic that aims to find good integer solutions early, helping eliminate branches with low objective values.
  * **Cuts** (`--cuts`): Allows `glpsol` to use all available cuts (Gomory, Mixed-Integer Rounding, Clique, Mixed Cover) to trim the search space.

<!-- end list -->

```bash
glpsol -m tiling.mod -d example.dat --fpump --cuts
```

These heuristics often help, sometimes dramatically, though not always. The output for the $30 \times 30$ instance with these options is:

```
Integer optimization begins...
Long-step dual simplex will be used
Gomory's cuts enabled
MIR cuts enabled
Cover cuts enabled
Number of 0-1 knapsack inequalities = 784
Clique cuts enabled
Constructing conflict graph...
Conflict graph has 896 + 1004 = 1900 vertices
+ 1896: mip = not found yet <= +inf (1; 0)
Applying FPUMP heuristic...
Pass 1
Solution found by heuristic: 153
Pass 1
Pass 2
Pass 3
Pass 4
Pass 5
Cuts on level 0: gmi = 12; clq = 1;
+ 2759: mip = 1.530000000e+02 <= 1.620000000e+02 5.9% (15; 0)
...
Time used: 60.2 secs. Memory used: 10.8 Mb.
+ 18551: mip = 1.530000000e+02 <= 1.620000000e+02 5.9% (225; 3)
TIME LIMIT EXCEEDED; SEARCH TERMINATED
Time used: 60.5 secs
Memory used: 13.1 Mb (13730993 bytes)
```

We see evidence of preprocessing for cuts, but the most significant improvement came from **feasibility pumping**, which found a solution of **153 tiles**. This is substantially better than 140, and the integrality gap dropped to **$5.9\%$**.

Note that if you are uncertain about complexity, you can omit time limits and manually stop `glpsol`, as it periodically reports the best solution. However, doing so skips the print commands at the end of the file. If `glpsol` stops on its own (e.g., due to a time limit), it sets variables to the current best solution and prints the output. Here is the reported solution with **153 tiles**:

```
Max. Cross Tiles (30x30): 153
..+....+.....+...+....+....+..
.+#+.++#+.+.+#+++#+.++#+.++#+.
..+++#++++#++++#++++#++++#+++.
.++#++++#+++#+.+++#++++#++++#+
+#++++#+++..+.++#++++#++++#++.
.+++#++++#+.++#++++#++++#+++..
.+#++++#++++#++++#++++#++++#+.
..+.+#++++#++++#++++#++++#+++.
..+..+++#++++#++++#++++#++++#+
.+#+++#++++#++++#++++#++++#++.
.+++#++++#++++#++++#++++#+++..
+#+.+++#++++#++++#++++#++++#+.
.++.+#++++#++++#++++#++++#+++.
.+#+.+++#++++#++++#++++#++++#+
..++.+#++++#++++#++++#++++#++.
.++#+.+++#++++#++++#++++#+++..
+#++.++#++++#++++#++++#++++#+.
.+.++#++++#++++#++++#++++#+++.
.++#++++#++++#++++#++++#++++#+
+#++++#++++#++++#++++#++++#++.
.+++#++++#++++#++++#++++#+++..
.+#++++#++++#++++#++++#++++#+.
..+++#++++#++++#++++#++++#+++.
.++#++++#++++#++++#++++#++++#+
+#++++#++++#++++#++++#++++#++.
.+++#++++#++++#++++#++++#++.+.
.+#++++#++++#++.+#++++#+++.+#+
..+++#++++#+++..++.+#++++#+++.
..+#++.+#++.+#++#+..+.+#+++#+.
...+....+....+..+......+...+..
```

The $30 \times 30$ square is almost perfectly filled. The gap between 153 and 162 remains open.

-----

## 7.3 Assignment Problem

This section presents another well-known optimization problem: the **assignment problem** [21].

-----

**Problem 40.**

Given $N$ workers and $N$ tasks, and knowing how well each worker can execute each task (described by a cost value), assign each worker exactly one task so that the **total cost is maximized** (or minimized).

As usual, we demonstrate the general problem through an example.

-----

**Problem 41.**

Solve Problem 40, the assignment problem, using the following data. There are $N = 7$ workers (W1 to W7), tasks T1 to T7, and the following cost matrix:

| | T1 | T2 | T3 | T4 | T5 | T6 | T7 |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| **W1** | 9 | 6 | 10 | 10 | 8 | 7 | 11 |
| **W2** | 7 | 12 | 6 | 14 | 10 | 5 | 5 |
| **W3** | 8 | 9 | 7 | 11 | 10 | 15 | 6 |
| **W4** | 4 | 10 | 2 | 10 | 6 | 4 | 7 |
| **W5** | 10 | 11 | 7 | 12 | 14 | 9 | 10 |
| **W6** | 5 | 9 | 8 | 9 | 13 | 3 | 8 |
| **W7** | 7 | 12 | 7 | 7 | 11 | 10 | 9 |

We can define the input sets and parameters in various ways. One possibility is to simply read $N$ and use numbers 1 to $N$. However, we want to allow naming from the data section, so we provide two sets: `Workers` and `Tasks`. A `check` statement ensures these sets have the same size.

Next, we introduce the `Assignments` set for all possible worker-task pairs, which helps formulation. The `Cost` parameter is defined for all assignments.

```
set Workers;
set Tasks;
check card(Workers)==card(Tasks);
set Assignments := Workers cross Tasks;
param Cost {(w,t) in Assignments};
```

Data for Problem 41 can be implemented as follows:

```
data;
set Workers := W1 W2 W3 W4 W5 W6 W7;
set Tasks := T1 T2 T3 T4 T5 T6 T7;
param Cost:
T1 T2 T3 T4 T5 T6 T7 :=
W1 9 6 10 10 8 7 11
W2 7 12 6 14 10 5 5
W3 8 9 7 11 10 15 6
W4 4 10 2 10 6 4 7
W5 10 11 7 12 14 9 10
W6 5 9 8 9 13 3 8
W7 7 12 7 7 11 10 9
;
end;
```

-----

There is only one kind of decision to be made: for each assignment, decide whether to assign that task to the worker or not. This is a binary variable named `assign`.

```
var assign {(w,t) in Assignments}, binary;
```

There are two rules we must obey: each worker must have exactly one task, and each task must have exactly one worker.

```
s.t. One_Task_Per_Worker {w in Workers}:
sum {t in Tasks} assign[w,t] = 1;
s.t. One_Worker_Per_Task {t in Tasks}:
sum {w in Workers} assign[w,t] = 1;
```

The objective is the total cost, obtained by adding each assignment variable multiplied by its cost.

```
minimize Total_Cost:
sum {(w,t) in Assignments} assign[w,t] * Cost[w,t];
```

Finally, we print the optimal cost and the task assigned to each worker.

```
set Workers;
set Tasks;
check card(Workers)==card(Tasks);
set Assignments := Workers cross Tasks;
param Cost {(w,t) in Assignments};
var assign {(w,t) in Assignments}, binary;
s.t. One_Task_Per_Worker {w in Workers}:
sum {t in Tasks} assign[w,t] = 1;
s.t. One_Worker_Per_Task {t in Tasks}:
sum {w in Workers} assign[w,t] = 1;
minimize Total_Cost:
sum {(w,t) in Assignments} assign[w,t] * Cost[w,t];
solve;
printf "Optimal Cost: %g\n", Total_Cost;
for {w in Workers}
{
printf "%s->", w;
for {t in Tasks: assign[w,t]}
{
printf "%s (%g)\n", t, Cost[w,t];
}
}
end;
```

The solution to the example problem is as follows (costs in parentheses):

```
Optimal Cost: 42
W1->T2 (6)
W2->T1 (7)
W3->T7 (6)
W4->T5 (6)
W5->T3 (7)
W6->T6 (3)
W7->T4 (7)
```

The smallest possible sum of assignments is 42.

Here are a few notes about the assignment problem:

  * The model is relatively simple. The number of feasible solutions for $N$ workers is $N!$ (factorial). For $N=7$, that's 5,040, which is manageable. However, for 20 tasks, $20! > 10^{18}$, making brute force impossible.
  * Mathematical programming handles large sizes well, but the **Hungarian method** [22] is a polynomial-time algorithm specifically designed for this problem that is substantially faster.
  * The **LP relaxation** of the assignment problem yields the optimal result for the MILP model. The integrality gap is guaranteed to be zero. Therefore, we don't even need binary variables; continuous variables between 0 and 1 suffice. This allows us to solve very large instances using LP.
  * The assignment problem is closely related to the transportation problem; it's effectively a transportation problem with $N$ sources and $N$ demands of 1 unit each.
  * Adding the same number to any row or column of the cost matrix doesn't change the optimal assignment, only the objective value.
  * Minimization and maximization are equivalent; negating costs turns one problem into the other.

Now, we will show an interesting extension: what happens when some decisions have already been made?

**Problem 42.**

Solve Problem 40 (the assignment problem) with **a priori decisions**: some assignments are explicitly declared to be either used or not used.

This technique isn't specific to the assignment problem; it applies to many real-world scenarios. Our goal is to support **a priori decisions** without breaking existing data files.

First, we define a parameter **`Fixing`** to express these decisions.

```
param Fixing {(w,t) in Assignments}, in {0,1,2}, default 2;
```

This parameter has three values:

  * **0**: **Exclude** the assignment.
  * **1**: **Force** this assignment to be used.
  * **2**: Leave the decision to **optimization**.

Setting the default to 2 ensures **compatibility with old data files** that don't mention `Fixing`.

Next, we enforce these decisions. For any `Fixing` value of 0 or 1, we explicitly set the `assign` variable to that value.

```
s.t. Fixing_Constraints {(w,t) in Assignments: Fixing[w,t]!=2}:
assign[w,t] = Fixing[w,t];
```

This completes the model. We demonstrate it with two trials.

-----

**Trial 1: Prohibiting an Assignment**

We set assignment **W6 to T6 as zero (excluded)**. Since the original optimal solution used this assignment, prohibiting it forces the solver to find an alternative.

```
param Fixing :=
W6 T6 0
;
```

```
Optimal Cost: 42
W1->T2 (6)
W2->T6 (5)
W3->T7 (6)
W4->T5 (6)
W5->T3 (7)
W6->T1 (5)
W7->T4 (7)
```

Even though W6-\>T6 was very cheap (cost 3), there is an alternative solution with the same cost of **42**.

-----

**Trial 2: Mandating an Assignment**

We set assignment **W4 to T3 as mandatory (1)**. This is the **cheapest assignment** in the matrix (cost 2) but wasn't used in previous optimal solutions.

```
param Fixing :=
W4 T3 1
;
```

```
Optimal Cost: 43
W1->T2 (6)
W2->T7 (5)
W3->T5 (10)
W4->T3 (2)
W5->T1 (10)
W6->T6 (3)
W7->T4 (7)
```

Surprisingly, forcing the cheapest assignment results in a **worse overall solution** (cost **43**). This demonstrates that simple **greedy heuristics** do not work perfectly for the assignment problem.

Note that a priori decisions can also be implemented by manipulating costs: giving an assignment a huge positive cost effectively excludes it, while a huge negative cost forces it.

-----

## 7.4 Graphs and Optimization

We will now look at two basic problems from **graph theory** that can be solved by MILP models.

A **simple graph** has **nodes** and **undirected edges**, where each edge connects two different nodes, and any two nodes are connected by at most one edge.

Some definitions required for the upcoming problems:

  * A **path** is a sequence of distinct nodes connected by edges. If the first and last nodes are connected, it forms a **cycle**.
  * A graph is **connected** if you can get from any node to any other via a path. Otherwise, it is **disconnected**.
  * A **tree** is a connected graph with no cycles.
  * A **spanning tree** of a graph is a subgraph that includes all nodes and is a tree. It's obtained by removing edges until no cycles remain but the graph stays connected.

If we assign a number (**edge weight**) to each edge, we get a **weighted simple graph**. We assume weights are positive.

**Problem 43.**

Solve the **shortest path problem** [24] on an arbitrary simple weighted graph: Given two nodes, find a path connecting them so that the **total weight of edges on the path is minimal**.

-----

**Problem 44.**

Solve the **minimum weight spanning tree (MST)** [25] problem: find the spanning tree of the graph with the **minimal total edge weight**.

-----

**Understanding the Problems**

In the **shortest path problem**, weights usually represent **distances**. In the **MST problem**, weights often represent **connection costs**. Since minimal connected spanning subgraphs are always spanning trees, the MST problem is essentially looking for the minimum weight connected graph.

Note that extremely efficient polynomial-time algorithms exist for these problems (e.g., **Dijkstra’s algorithm** [26] for shortest paths, **Kruskal’s** [27] or **Prim’s algorithm** for MST). However, MILP models can be easier to formulate and adapt to complex variations.

**Figure 12** shows an example graph used for demonstration.

-----

**Problem 45.**

On the graph in **Figure 12**, find the shortest path between nodes **A and I**, and find the minimum weight spanning tree.

**Data Implementation**

We first implement a data section. The set **`Nodes`** contains the nodes. We define **`Start`** and **`Finish`** nodes for the shortest path. **`Weight`** describes edge weights.

```
data;
set Nodes := A B C D E F G H I;
param Start := A;
param Finish := I;

param Weight :=
A B 5
B C 7
A D 3
...
F H 2
;
end;
```

Parameters **`Start`** and **`Finish`** are symbolic and must be in the `Nodes` set. We assert they are different.

```
set Nodes;
param Start, symbolic, in Nodes;
param Finish, symbolic, in Nodes;
check Start!=Finish;
```

While edges in simple graphs are undirected ($AB = BA$), it is easier in math programming to refer to them as **$(A, B)$ ordered pairs**. We allow all ordered pairs (directed arcs) because edge direction helps in the model.

We introduce a parameter **`Infty`** as a default large cost to effectively exclude edges not listed in the data.

```
param Infty, default 99999;
param Weight {a in Nodes, b in Nodes}, >0, default Infty;
param W {a in Nodes, b in Nodes} := min(Weight[a,b],Weight[b,a]);
```

Parameter **`W`** ensures that if we provide a weight for $AB$, it applies to both $AB$ and $BA$.

**Solving the Shortest Path Problem**

The idea is to imagine a single **"droplet" of material** placed at **`Start`**. It flows through the arcs to reach **`Finish`**. This draws the path.

A binary variable **`flow`** denotes whether the droplet flows through an arc $(a, b)$.

```
var flow {a in Nodes, b in Nodes}, binary;
```

We need to check the **material balance** at each node:

  * **Start**: Droplet leaves. Balance is **-1**.
  * **Finish**: Droplet arrives. Balance is **1**.
  * **Others**: Droplet arrives and leaves. Balance is **0**.

<!-- end list -->

```
subject to Path_Balance {x in Nodes}:
sum {a in Nodes} flow[a,x] - sum {b in Nodes} flow[x,b] =
if (x==Start) then -1 else if (x==Finish) then 1 else 0;
```

This ensures a **feasible trail** exists. The objective is to minimize total weight.

```
minimize Total_Weight:
sum {a in Nodes, b in Nodes} flow[a,b] * W[a,b];
```

Optimization will naturally rule out cycles and extra edges because of their positive weight, resulting in a simple path.

Solving for **A to I** gives:

```
Distance A-I: 19
A->D (3)
B->E (4)
D->B (1)
E->H (2)
H->I (9)
```

The path is **19** (A-D-B-E-H-I). Note that the shortest path isn't necessarily the one with the fewest edges. Like the assignment problem, the **LP relaxation** of this model yields the optimal solution, so it could be a pure LP.

-----

**Solving the Minimum Weight Spanning Tree (MST) Problem**

We use a similar flow strategy. We want a connected graph. Let's put a droplet in **each node** and have them all flow to a single **sink node** (e.g., A).

  * If the graph is connected, droplets can reach the sink from everywhere.
  * If disconnected, they cannot.

<!-- end list -->

```
param Sink := A;
```

We use two variables:

  * **`use`**: Binary. Is the edge selected?
  * **`flow`**: Continuous. How much material flows through it?

<!-- end list -->

```
var use {a in Nodes, b in Nodes}, binary;
var flow {a in Nodes, b in Nodes};
```

Constraints ensure flow in one direction is the negative of the other, and flow only happens on selected (`use`) edges (using a big-M constraint based on node count).

```
subject to Flow_Direction {a in Nodes, b in Nodes}:
flow[a,b] + flow[b,a] = 0;
subject to Flow_On_Used {a in Nodes, b in Nodes}:
flow[a,b] <= use[a,b] * (card(Nodes) - 1);
```

Material balance:

  * **Sink**: Receives all droplets ($\mathbf{1 - \text{card(Nodes)}}$).
  * **Others**: Sends one droplet ($\mathbf{1}$).

<!-- end list -->

```
subject to Material_Balance {x in Nodes}:
sum {a in Nodes} flow[a,x] - sum {b in Nodes} flow[x,b] =
if (x==Sink) then (1-card(Nodes)) else 1;
```

Minimizing the total weight of `use`d edges results in a spanning tree directed toward the Sink.

Solving the example with Sink **A**:

```
Cheapest spanning tree: 31
A<-D (3)
B<-E (4)
D<-B (1)
E<-H (2)
F<-C (6)
F<-I (8)
H<-F (2)
H<-G (5)
```

The total weight is **31**. Unlike shortest path, this **cannot be relaxed to LP** because `use` must remain binary to capture the fixed cost of an edge regardless of flow volume.

-----

## 7.5 Traveling Salesman Problem

Here we present the **Traveling Salesman Problem (TSP)** [28], a notoriously difficult problem.

[Image of traveling salesman problem]

**Problem 46.**

Given a set of **nodes** and **distances** between them, find the **shortest cycle** that **visits all nodes**.

The goal is to visit a set of targets and return to the start with minimal travel. This optimal route is a **Hamiltonian cycle**. The starting node can be arbitrary.

Compared to the knapsack problem, TSP is generally harder to solve for the same number of elements. We assume equal distances in both directions. For simplicity, we solve a TSP for nodes on a **plane** using **Euclidean distances**.

-----

**Problem 47.**

Find the **shortest route** starting and ending at the **green node** and visiting all **red nodes** in Figure 15.

We implement a model that accepts planar positions and calculates distances. The nodes are listed in a set `Node_List`.

```
data;
set Node_List :=
P00 0 0
P08 0 8
...
P97 9 7
;
param Start := P23;
end;
```

In the model, `Node_List` is a three-dimensional set (name, x, y). We derive `Nodes`, `X`, and `Y` from it. We calculate Euclidean distance `W` using `sqrt` and `^2`.

**Solution Strategy:**

TSP is closely related to the **assignment problem**. Each node must have exactly one "next" node in the cycle. This is an assignment from the set of nodes to itself.

```
var use {a in Nodes, b in Nodes}, binary;
subject to Path_In {b in Nodes}: sum {a in Nodes} use[a,b] = 1;
subject to Path_Out {a in Nodes}: sum {b in Nodes} use[a,b] = 1;
```

However, simple assignment constraints might result in **multiple smaller cycles** rather than one big cycle (see Figure 17). To prevent this, we must ensure **connectivity**. We reuse the MST flow technique: put a droplet at each node and ensure they can all flow to `Start`. This is possible if and only if there is a single connected cycle.

```
var flow {a in Nodes, b in Nodes};
...
subject to Material_Balance {x in Nodes}: ...
```

The TSP model is essentially a combination of the **assignment problem** and **connectivity constraints**.

Solving Problem 47 yields a cycle length of **44.59**.

```
Shortest Hamiltonian cycle: 44.5948
P00->P31 (3.16228)
...
P97->P69 (3.60555)
```

Even with only 19 nodes, solving takes time (about 9 seconds here).

**SVG Output**

To visualize the result, the model generates an **SVG image**. We translate TSP coordinates to image pixels, define a file parameter, and use `printf` to write XML tags (rectangles for background/nodes, lines for the grid/path). This allows `glpsol` to produce a graphical solution file directly.

-----

## 7.6 MILP Models – Summary

This chapter demonstrated how **Mixed-Integer Linear Programming (MILP)** solves problems involving discrete decisions.

  * **Knapsack & Partitioning:** Basic examples of discrete choices using integer variables.
  * **Tiling:** Solved by defining sets for valid placements.
  * **Assignment Problem:** Implemented as an MILP, though its LP relaxation works perfectly. We also showed how to implement **a priori decisions**.
  * **Graphs:** **Shortest Path** and **MST** were solved using **flow/material balance** concepts.
  * **TSP:** Solved by combining assignment logic with connectivity constraints. We also generated **visual SVG output**.

**Integer programming** expands the range of solvable problems significantly compared to pure LP, but it comes with a steep increase in **computational complexity**.