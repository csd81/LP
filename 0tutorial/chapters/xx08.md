

## Problem 7.

Given a system of linear equations, find a solution that minimizes the maximum error across all equations. The error for a specific equation is defined as the absolute value of the difference between the Left-Hand Side (LHS) and the Right-Hand Side (RHS).
(The LHS of the equation consists of the sum of the variables multiplied by their coefficients, while the RHS is a constant. Use the same data description method as before.)

This is no longer a feasibility problem. To be precise, all possible variable values now provide a feasible solution; strictly speaking, the objective value (maximum error) simply varies between solutions. Therefore, this is an optimization problem.

Solving this efficiently requires a modeling trick: minimizing the maximum objective. This can be considered a "design pattern" in mathematical programming.

The first part of this concept involves introducing the objective—the maximum error itself—into the model as an individual variable, as follows:

```
var maxError;
minimize maxOfAllErrors: maxError;
```

At this point, the objective is simply an independent variable that can be set freely. Our goal is to find a way to enforce this `maxError` variable to effectively represent the maximum error of the equations.

Suppose that $L$ and $R$ are the left-hand side and right-hand side values of an equation for certain variable values. Ideally, $L = R$, and the error is zero. Otherwise, the error is $|L - R|$, which is not a linear function of the variables. Even if it were, finding the maximum of these values would still be problematic because the "maximum" function itself is not linear.

This leads to our second idea. Instead of representing the errors themselves in the model, we use an upper bound for the error. Suppose that $E$ is a valid upper bound for the error between $L$ and $R$, meaning $|L - R| \le E$. This can now be expressed as linear constraints as follows:

```
L - R <= E
L - R >= -E           (11)
```

Note that $E$ must be nonnegative, as the errors themselves are nonnegative. Alternatively, we could state $R - E \le L \le R + E$. In any case, these represent two linear inequalities.

Now, what would be a good candidate for an upper bound on the error in any equation? Naturally, the maximum of the errors, which is denoted by `maxError`. Therefore, we define two constraints so that the two sides of the equations differ at most by the value of `maxError`.

```
s.t. Cts_Error_Up {e in Equations}:
  sum {u in UnknownValues} Coef[e,u] * value[u]
  <= Rhs[e] + maxError;

s.t. Cts_Error_Down {e in Equations}:
  sum {u in UnknownValues} Coef[e,u] * value[u]
  >= Rhs[e] - maxError;
```

This is achieved by duplicating the original constraint and including `maxError`. The constraints must have unique names; here, `Cts_Error_Up` and `Cts_Error_Down` were chosen.

Surprisingly, our model is complete. Let us now try to understand what happens when it is solved.
Due to the constraints, `maxError` is forced to act as a valid upper bound for the errors of all equations simultaneously. Meanwhile, `maxError` is an objective to be minimized. Consequently, these factors combined will effectively find the smallest possible `maxError` value for which all equations can be satisfied.

Below is the full model section, with some `printf` statements added to provide meaningful output.

```
set UnknownValues;
set Equations;
param Rhs {e in Equations};
param Coef {e in Equations, u in UnknownValues}, default 0;
var value {u in UnknownValues};
var maxError;

s.t. Cts_Error_Up {e in Equations}:
  sum {u in UnknownValues} Coef[e,u] * value[u]
  <= Rhs[e] + maxError;

s.t. Cts_Error_Down {e in Equations}:
  sum {u in UnknownValues} Coef[e,u] * value[u]
  >= Rhs[e] - maxError;

minimize maxOfAllErrors: maxError;

solve;

printf "Optimal error: %g\n", maxError;
printf "Variables:\n";
for {u in UnknownValues}
{
  printf "%s = %g\n", u, value[u];
}

printf "Equations:\n";
for {e in Equations}
{
  printf "%5s: RHS=%10f, actual=%10f, error=%10f\n",
  e, Rhs[e],
  sum {u in UnknownValues} Coef[e,u] * value[u],
  abs(sum {u in UnknownValues} Coef[e,u] * value[u] - Rhs[e]);
}
end;
```

If we solve it, we get the following results:

```
Optimal error: 0.478261
Variables:
x = -3.42029
y = -1.21884
z = 2.24058
w = 3.23768
v = -0.36087
Equations:
Eq1: RHS= -5.000000, LHS= -4.521739, error= 0.478261
Eq2: RHS= -5.000000, LHS= -5.478261, error= 0.478261
Eq3: RHS=  1.500000, LHS=  1.021739, error= 0.478261
Eq4: RHS=  0.000000, LHS= -0.478261, error= 0.478261
Eq5: RHS= -0.500000, LHS= -0.021739, error= 0.478261
Eq6: RHS=  0.000000, LHS=  0.478261, error= 0.478261
```

If we manually investigate the results, we can verify that all the variables are multiples of $1/690$; they are not trivial values.

It is interesting to note that the error is the same for all six equations. Perhaps this is a general rule when the number of equations is one greater than the number of variables? This leads to another mathematical problem.

### 4.9 Equation Systems – Summary

We have learned the basic skills in GNU MathProg required to implement linear mathematical models: using parameters, separating the model and data sections, and, most importantly, indexing expressions. We also solved a simple yet non-trivial optimization problem regarding minimizing errors in equations.

Note that the coding effort was minimal in GNU MathProg—and mathematical programming in general—compared to solution algorithms we would otherwise have to implement ourselves.

From now on, it is recommended that for each subsequent optimization problem, you implement a single general model file containing the model section but no problem data. Each problem instance for which the model needs to be solved can be implemented in a separate data file containing the entire data section. Therefore, there should be as many data files as there are problem instances.

