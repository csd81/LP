
## ðŸ’¡ Advanced GMPL Techniques 2: Handy Tips and Tricks

### Default Values, Display Statements, Derived Parameters, etc.

-----

### Prerequisites

*Advanced GMPL Techniques: Data/Logic Decoupling*

This lesson introduces several handy GMPL language features that can significantly improve your modeling experience. While none of these features enable you to solve problems you couldn't solve before, they are designed to make writing, understanding, and debugging a model much **easier**. Remember, you are never *forced* to use any of these techniques. If you don't feel the need, you probably shouldn't. These tricks are available to simplify your work, not to complicate it.

We will use all of these features occasionally later on, so you should be able to **understand** them, even if you choose not to implement them yourself.

-----

### Default Values for Parameters

Imagine you have a model that you use with hundreds of data files. Within these files, there is a zero-dimensional parameter, such as a **tax rate**, that has the same value in almost all of them, except for a select few.

Based on what we've learned so far, you have two options:

1.  **Provide the value for that parameter in all data files.**
2.  **Create a separate model** where that data is fixed to a specific value and use it for the majority of your data files.

Option 1 is the obvious path, but repeatedly specifying the same value is not the most elegant solution. Data files often share the same parameter value because they are tied to the same real-world scenario (like the tax rate in a specific country). If that real-world value changes, you must update *all* your data files.

Option 2 isn't much better. If the model's constraints or variables change, you must keep two separate model files updated. Keeping them synchronized is problematic, and you also need to remember which model to use with which data file.

A much cleaner approach is to define a **default value** for the parameter:

  * If the value is **not provided** in the data file, the parameter will take the default value.
  * If the value **is provided** in the data file, it will **override** the default.

This allows most of your models to omit the value, while others can still redefine it. Furthermore, if the value changes, you only have to update it in one place.

The basic syntax is very simple:
`param Pname{...} default numericvalue;`

Defaults can be useful in other situations as well. Suppose you have a one-dimensional parameter that is the same for most indices. For example, an **initial stock** parameter that is zero for most products, except for a few, when planning weekly production. It would be helpful to only list the values for those non-zero products and have all others automatically set to $0$. This type of default is useful when the parameter has a **"natural default value."**

To achieve this effect, the model section would contain:

```
set Products;
param initial_stock{Products} default 0;
```

And the data file could look like this:

```
set Products:= p1 p2 p3 p4 p5 p6 p7 p8 p9 p10;
param initial_stock:=
  p3  100
  p7  45
  ;
```

This also works well for two-dimensional parameters. Imagine you are scheduling machines and have a parameter for the time a machine requires to perform a task. Not all machines can carry out all tasks, which you want to model by assigning a very large number for the processing time.

The model section is straightforward:

```
set Tasks;
set Machines;
param processing_time{Tasks,Machines} default 1000;
```

But how do you specify which values in a matrix you want to provide? Simply omitting the values makes the data section ambiguous, so you must put a **dot** (`.`) in the places where you don't want to provide a value:

```
set Tasks := t1 t2 t3 t4 t5;
set Machines := m1 m2 m3;
param processing_time :
      m1  m2  m3:=
  t1  1   .   2
  t2  3   .   .
  t3  1   2   3
  t5  .   5   4
  ; 
```

Notice that `t4` is omitted from the list; this is equivalent to adding a line that says `t4 . . .`. These dots and default values can be used in more complex scenarios, but for now, we will limit their use to these examples.

-----

### Transpose

A common beginner mistake is to define a two-dimensional parameter and confuse its indices, often resulting in "out of domain" error messages. While replacing $p[b,t]$ with $p[t,b]$ throughout the constraints can be tedious, it is manageable.

A more significant problem arises if you provided the values in the data section in the reversed order. This is especially challenging for large tables.

For a small example:

```
set A;
set B;
param p{A,B};
...

data;

set A := a1 a2 a3 a4;
set B := b1 b2 b3;

param p :
      a1  a2  a3  a4 :=
  b1  12  34  54  12
  b2  98  87  76  65
  b3  78  65  67  43
  ;    
```

You have several options:

1.  **Manually** (or with an external tool) transpose the matrix in the data section.
2.  Change the order of the indices in the declaration and everywhere else in the model.
3.  **Place `(tr)` after `param p` and before the colon in the data section.**

I'll leave the decision to you as we move on to the next parameter tweak.

-----

### Parameter Safety Checks

If you know a parameter must always have a certain type of value, you should include that check immediately after its declaration. For instance, the **processing time** mentioned earlier obviously cannot be negative, so a declaration like `param processing_time{Tasks,Machines}>=0;` is better than the previous one.

This doesn't change the underlying logic, but you will receive an error message if, by accident, an unrealistic value is provided in one of the data files. This is a simple **safety check** for a common mistake.

-----

### Measurement Units

It is always good practice to include **measurement units** in comments next to declarations and definitions to avoid major issues. However, you might encounter a parameter that is often received in one unit for some data files and a different unit for others. If you use only one of these units in your constraints, you might not want to maintain separate model files for the two input types.

A simple GMPL **hack** can solve this, as demonstrated by the following declaration:

```
set Cities;
param distance_mile{Cities,Cities} >= 0, default 0;
param distance_km{c1 in Cities, c2 in Cities} >=0, default distance_mile[c1,c2]*1.6;
```

You would then use `distance_km` in all your constraints.

  * In files using **metric units**, you can provide `distance_km`. No conversion will be done, and the dot (`.`) will mean $0 \times 1.6$, which is still $0$.
  * In files using **imperial units**, you can use `distance_mile`. All values for `distance_km` will be calculated based on the `distance_mile` values.

A small warning: If you don't provide *either* parameter, the distances will be $0$ everywhere.

-----

### Displaying the Results in a User-Friendly Format

We've seen in previous examples (like the generic "FrÃ¶ccs" model) that the optimal solution often results in most variables taking on a "non-interesting" value (e.g., producing $0$ portions of most drink types).

In industrial-scale problems, this can be even more severe, with hundreds of variables, and only a handful taking a non-zero value. It would be helpful to generate a simple list of those variables and their values. Fortunately, GMPL supports this via a **"display section"** placed between the model and data sections:

```
set FroccsTypes;
...
var quantity{FroccsTypes} >= 0;
...
maximize Income: sum {f in FroccsTypes} price[f]*quantity[f];

solve;

printf "Overall profit: %g\n\n", Income;

for{f in FroccsTypes : quantity[f]!=0}
{
  printf "Produce %g portions from %s.\n",quantity[f],f;
}

data;

...
```

If you run the model with this, you will see output on the standard console similar to this:

```
Overall profit: 115625

Produce 937.5 portions from kisfroccs
Produce 62.5 portions from soherfroccs
```

The first critical element is the **`solve;`** keyword, which notifies the solver that the model description has ended and it must solve the model before proceeding. This ensures that in the display section, the "variables" are no longer variables; they hold fixed valuesâ€”the values from the optimal solution.

**`printf`** is a commonly used command for formatted output. If you have a background in C, bash, or similar languages, this syntax will be familiar. If not, it is highly recommended to study the `printf` syntax. For the basics:

  * The first string after `printf` is what the command should output.
  * A percentage symbol followed by a character (`%g`, `%s`, etc.) is a special marker replaced by the next item listed.
  * `%s` is used for strings, `%d` for decimals (integers), and `%g` for non-decimals (floating-point numbers). `printf` is powerful, so check the documentation for more options.

The other key structure is the **`for`** loop, which is a `foreach` type of loop. It iterates through all elements of the set (in this case, `FroccsTypes`) using a dummy index (`f`). After the colon, you can add **filters**. In this example, the filter `quantity[f]!=0` expresses that the loop should only go through drink types that are actually produced in some quantity.

*Note:* Similar filters can be used for constraints and `sum` statements, but only with parameters. Unlike the display section, variables do not have a fixed value in the main model section.

The `for` loop above is equivalent to an `if` statement inside a standard loop, conceptually:

```
for {f in FroccsTypes}
{
  if {quantity[f]!=0}
  {
    printf "....
  }
}
```

While using a filter is simpler here, sometimes an actual conditional is needed. Unfortunately, GMPL lacks a formal `if` element, but there is a non-standard workaround using loops:

```
for { {0}: quantity[f]!=0}
{
  ....
}
```

This essentially loops through the single element of the set `{0}` and checks the condition for each. The set has no real connection to the condition, and any **singleton set** would work. It's not the cleanest syntax, but it works.

-----

### Output as Input for a Subsequent Tool

The formatted output is great for human readability. However, you might want another software tool to use the data from the optimal solution. A simple example is generating a table that can be copied directly into Excel or another spreadsheet program. This can be done by separating all the outputted values with **tabs** and then copy-pasting the output from the terminal.

Copying huge tables from the terminal is often painful, prone to mistakes, and the output is mixed with the solver's standard log messages. It would be helpful to redirect our specialized outputs to a separate file. This is exactly what the **`-y` command line option** is for.

Run `glpsol -m model.mod -d data.dat -y myoutput.txt`, and everything you printed will be "copy-pasted" into that file. If you plan to use this output in Excel, you should create a CSV (Comma Separated Values) file using these statements.

These statements are quite powerful. They have been used to generate:

  * CSV format for spreadsheet software like LibreOffice Calc.
  * **LaTeX table code** (since LaTeX tables are cumbersome to write manually).
  * **SVG image files** to better visualize a solution.

Any text-based output can be generated easily. You can even generate another model or data file if you have a hierarchical optimization process, or run `glpsol` from a script that then calls a script generated by the display statement.

-----

### Final Notes

The GMPL techniques introduced in this lesson are handy tweaks that can often make the modeling or interpretation process more **transparent and easy**. However, once again, only use them if you feel they help you, not if they make things more complicated.

 